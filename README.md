# PySpark-SQL-Home_Sales


In this challenge, you'll use your knowledge of SparkSQL to determine key metrics about home sales data. Then you'll use Spark to create temporary views, partition the data, cache and uncache a temporary table, and verify that the table has been uncached.

The Home_Sales jupyter notebook is in the Starter_Code folder and was done in Google Colab.
